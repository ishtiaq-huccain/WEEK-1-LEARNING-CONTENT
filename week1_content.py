# -*- coding: utf-8 -*-
"""week1_content.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1l108ICqqUlX-5brX8R3PcZRV1TxwUeK3

## Ishtiaq Hussain
## WEEK 1 LEARNING (Emumba)

#### Intro to GenAI:  
Subset of deep learning, in which the model is able to generate new data after being trained on various data.  


Types of GenAi models:  
- Text to text: Gpt 3, Claude, Mistral  
- Text to image: Gpt 4o, Dall-e, Midjourney  
- Text to Video: Veo3 by Google  
- Text to Audio: Trillet.ai, ElevenLabs  
- Text to Task: Software agents like n8n, make.com etc   

### LLM'S:
- Advance ai models trained on massive amount of text data to understand and generate human like language.  
- They work by prediction next word in a sentence using patterns learned during training.  
- Most modern LLMs use the Transformer model.

### LLM Parameters:
- Controls the output of the LLM's.  
- Parameters includes weight, biases attentions in transformer.  
- Initially set random, then we fine tuned these parametrs to predict next word with greater probability using tecniques likes backpropagation.
- In backpropagation, we update the the weights and biases of the neural network to reduce the difference between actual output and predicted output of the model using techniques like Gradient descent, stochastic gradient descent, MSE.  


- GPT-3 has 175B parameters.
- GPT 4o has 1.8T parameters.
- Grok 3 has 2.7T parameters (Trained on real-time data X(Twitter))

### LLM Settings:  
- Temperature: It regulates the unpredictability of the model's output.  
     - High Temp:
        - model takes more risks  
        - more random and creative (usefull for story generation, poetry) in simple when you want more context.  
     - Low Temp:  
        - fully deterministic, more confident output, choose prediction wisely.  
        - Useful for code generation, Solving optional Mcq's, legal documents inqueries.  
- Top-P: Top p% more likely words like words having probability (0.8-1)  
- Stop Sequence: Stop when certain tokens appears  
- Presence Penalty: Penalize words that have already been appeared.
- Frequency Penalty: Penalize frequently used words.

### Prompt Engineering:  
- Refining our Prompts to improve the performance of Generative ai models.
- Why prompts: Generative AI models heavily rely on the inputs provided by the user.  

   ##### Types of Prompts:  
   - Zero-Shot prompting:
      - Just a clear instruction e.g: Translate the following sentence into Urdu:
        "How are you?"
   - Few-Shot prompting:
      - Provide a few examples to teach the pattern.  
      
        
      E.g: Translate to French:
            English: "Good morning"  
            French: "Bonjour"

            English: "I love you"  
            French: "Je t'aime"

            English: "See you later"  
            French:  
   - Chain of Thought:  
      - Encourage the model to show reasoning steps.  
      - E.g: If a train leaves at 3 PM and takes 2 hours to reach the destination, what time does it       arrive?  
      Let's think step by step.
      - Best for mathematical calculations.  

   - Role based Prompting:  
      - Ask the model to act as someone.  
      - E.g: You are a senior software engineer. Explain recursion in simple terms.  
   - Contextual Prompting:
      - Provide background information or documents before the task.
      - E.g: [Paste the hooks for the Upwork proposal writing]  
            Write a proposal for the job.

### Groq Api Keys: (Not Grok)
- I was a little confused at first.
- Despite the similiar sounding names "Grok" and "Groq" are completely different things developed by two different companies.
    - Grok by xAI (Elon Musk)
    - Groq by Groq Inc.
- #### Some of the Groq open models:
    - LLama-3 (Text to Text):
        - meta-llama/Meta-Llama-3-8B-Instruct
    - Mistral Ai (Multilingual):
        - mistralai/Mixtral-8x7B-Instruct-v0.1
    - Gemma-2 (Text to Text):    
        - google/gemma-7b-it
    - Speech to Text (WhisperAI):  
        - model=whisper-large-v3  
    -  Qwen QWQ (Reasoning):
        - model=qwen-qwq-32b  
    - PlayAi Dialog (Text to speech):
        - model=playai-tts

### MICROSOFT PHI-2 MODEL:
"""

!pip install transformers accelerate --upgrade -q

from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

model_id = "microsoft/phi-2"

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto")

pipe = pipeline("text-generation", model=model, tokenizer=tokenizer)

prompt = "Explain black holes to a 10-year-old."
output = pipe(prompt, max_new_tokens=100)

print(output[0]["generated_text"])

# Use Case: Sentiment Analysis
text = "I absolutely love the new features on this phone!"

# Zero-shot Prompt
prompt1 = f"What is the sentiment of this text?\n\"{text}\""

# Few-shot Prompt
prompt2 = f"""
Determine the sentiment of the following texts:
"Terrible service and rude staff." â†’ Negative
"This meal was amazing!" â†’ Positive
"{text}" â†’"""

# Chain-of-Thought Prompt
prompt3 = f"""
Analyze the sentiment step by step.
Text: "{text}"
Step 1: Identify key emotional words â†’ "absolutely love", "new features"
Step 2: "Absolutely love" is highly positive.
Conclusion: The sentiment is Positive.
"""

# Store and evaluate all prompt types
prompt_versions = {
    "Zero-shot": prompt1,
    "Few-shot": prompt2,
    "Chain-of-Thought": prompt3
}

for technique, prompt in prompt_versions.items():
    output = pipe(prompt, max_new_tokens=100, temperature=0.7, do_sample=False)
    print(f"\n {technique} Prompt Output:\n{output[0]['generated_text']}")

"""### RUN GROQ API MODEL (PLAYAI DIALOG)  
### TEXT-TO-SPEECH MODEL
"""

!pip install groq -q

!pip install groq -q

from groq import Groq
from IPython.display import Audio
from io import BytesIO

# Insert your API key here
client = Groq(api_key="")

# Request TTS
response = client.audio.speech.create(
    model="playai-tts",
    voice="Aaliyah-PlayAI",
    input="Hi, this is my first week of learning GenAI and backend development at Emumba.",
    response_format="wav"
)

audio_buffer = BytesIO(response.read())

Audio(audio_buffer.read(), autoplay=False)

import nbformat

file_path = "week1_content.ipynb"  # Replace with your actual file name

with open(file_path, "r", encoding="utf-8") as f:
    nb = nbformat.read(f, as_version=4)

# âœ… Remove broken widget metadata
if "widgets" in nb.metadata:
    del nb.metadata["widgets"]

# ðŸ’¾ Save the cleaned notebook
with open(file_path, "w", encoding="utf-8") as f:
    nbformat.write(nb, f)

print("âœ… Cleaned and saved. Re-upload to GitHub.")

